{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar um agente de aprendizado que otimize a experiência do usuário neste site, aumentando o número de conversões por meio de recomendações personalizadas, podemos adotar uma abordagem baseada em *Aprendizado por Reforço*, mais especificamente, _Reforço Profundo (Deep Reinforcement Learning)_ utilizando o Processo de Decisão de Markov (MDP).\n",
    "\n",
    "Irei descrever brevemente como modelar esse problema, considerando suposições e estratégias sobre os dados e comportamento dos usuários, bem como para coleta de dados adicionais.\n",
    "\n",
    "### 1. Definição do Problema\n",
    "\n",
    "O objetivo é criar um agente que aprenda a recomendar produtos ou conteúdos aos usuários, maximizando o número de conversões (transações).\n",
    "As recomendações devem ser personalizadas com base no comportamento de navegação e compras dos usuários no site.\n",
    "\n",
    "### 2. Modelagem do Problema\n",
    "\n",
    "A modelagem do problema envolve a definição de variávies que podem ser entedidas a rigor como estados, ações e funções de recompensa, modelando Cadeias de Markov e aplicando o MDP sobre tais cadeias. Não entrarei aqui em detalhes estatísticos e matemáticos dessa abordagem e irei prezar por uma explicação mais simples e prática voltada a um público mais amplo.\n",
    "\n",
    "#### Ambiente\n",
    "\n",
    "O ambiente é o site de e-commerce, onde os estados são definidos pelas interações dos usuários com o site. Cada interação inclui informações como:\n",
    "\n",
    "* Páginas visitadas;\n",
    "* Produtos visualizados;\n",
    "* Tempo gasto em cada página;\n",
    "* Dispositivo e sistema operacional utilizado;\n",
    "* Histórico de transações.\n",
    "\n",
    "#### Estados\n",
    "\n",
    "Os estados podem ser representados por um vetor de características que descrevem o comportamento atual do usuário. Por exemplo:\n",
    "\n",
    "* Produtos recentemente visualizados;\n",
    "* Categorias de interesse;\n",
    "* Dispositivo utilizado (iOS, Android, Desktop);\n",
    "* Tempo de permanência no site;\n",
    "* Navegações anteriores (páginas visitadas, produtos adicionados ao carrinho).\n",
    "\n",
    "#### Ações\n",
    "\n",
    "As ações são as recomendações feitas pelo agente. Isso pode incluir:\n",
    "\n",
    "* Recomendação de produtos específicos;\n",
    "* Ofertas personalizadas;\n",
    "* Sugestão de conteúdo relevante (artigos, reviews);\n",
    "* Recompensas.\n",
    "\n",
    "A função de recompensa deve incentivar ações que levam a uma conversão. Por exemplo:\n",
    "\n",
    "* +1 para cada conversão (transação completada);\n",
    "* -0.1 para cada recomendação que não resulta em clique;\n",
    "* +0.5 para ações que aumentam o tempo de permanência no site.\n",
    "\n",
    "### 3. Suposições\n",
    "\n",
    "*Dados Históricos:* Supomos que temos acesso a um histórico detalhado das interações dos usuários com o site.\n",
    "*Dados em Tempo Real:* É necessário coletar dados em tempo real para ajustar as recomendações dinamicamente.\n",
    "*Capacidade de Personalização:* O site tem a capacidade de personalizar a interface e as recomendações com base nas ações do agente.\n",
    "\n",
    "### 4. Coleta de Dados Adicionais\n",
    "\n",
    "Para melhorar as recomendações, poderíamos coletar dados adicionais como:\n",
    "\n",
    "* Feedback explícito dos usuários sobre recomendações (curtidas, descurtidas);\n",
    "* Dados de contexto (localização geográfica, hora do dia);\n",
    "* Comportamento em redes sociais (se o usuário se conecta via redes sociais);\n",
    "* Informações provenientes de fontes externas como o score de crédito e outros dados relevantes obtidos na API do Sistema de Proteção ao Créito (SPC).\n",
    "\n",
    "### 5. Implementação da Função de Recompensa\n",
    "A função de recompensa pode ser implementada como:\n",
    "\n",
    "```python\n",
    "def calcular_recompensa(transacao_completa, recomendacao_clicada, tempo_permanencia):\n",
    "    recompensa = 0\n",
    "    if transacao_completa:\n",
    "        recompensa += 1\n",
    "    if recomendacao_clicada:\n",
    "        recompensa += 0.5\n",
    "    else:\n",
    "        recompensa -= 0.1\n",
    "    if tempo_permanencia > tempo_limiar:\n",
    "        recompensa += 0.5\n",
    "    return recompensa\n",
    "\n",
    "```\n",
    "\n",
    "#### 6. Estrutura do Agente de Aprendizado\n",
    "\n",
    "Para treinar o agente, podemos usar uma Rede Neural Profunda que aprende a mapear estados para ações maximizando as recompensas esperadas. Existem na literatura vários algoritmos de aprendizado por reforço que podem ser adaptados para o contexto específico do nosso problema, como Q-Learning, Deep Q-Networks (DQN), Policy Gradient, etc.\n",
    "\n",
    "### 7. Avaliação e Otimização\n",
    "\n",
    "Após treinar o agente, é necessário avaliar seu desempenho em um ambiente simulado ou em produção. Podemos usar métricas como Taxa de Conversão, Tempo Médio de Permanência, Taxa de Cliques, entre outras. O agente deve ser continuamente otimizado com base no feedback dos usuários, nos dados coletados e nas métricas de desempenho. \n",
    "\n",
    "É importante notar que, em um ambiente de produção, campanhas como Black Friday, Dia das Mães e outras podem influenciar no comportamento dos usuários e, portanto, o agente deve ser capaz de se adaptar a essas mudanças.\n",
    "\n",
    "### 8. Algoritmo de Treinamento\n",
    "\n",
    "1. **Inicialização:** Inicialize a rede neural com pesos aleatórios.\n",
    "2. **Exploração vs Exploração:** Utilize uma política ε-greedy para balancear entre exploração (experimentar novas ações) e exploração (usar ações conhecidas).\n",
    "3. **Atualização da Q-Function:** A cada interação, atualize a Q-Function usando o algoritmo Q-Learning:\n",
    "\n",
    "```python\n",
    "    Q(s, a) = Q(s, a) + α [r + γ max_a Q(s, a) - Q(s, a)]\n",
    "```\n",
    "4. **Replay Memory:** Utilize uma memória de replay para armazenar interações e amostrar mini-batches para treinamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, self.action_size)\n",
    "        )\n",
    "        model.compile(loss='mse', optimizer=optim.Adam(model.parameters(), lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.model(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        q_values = self.model(states)\n",
    "        next_q_values = self.model(next_states)\n",
    "        q_values_target = q_values.clone()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            target = rewards[i]\n",
    "            if not dones[i]:\n",
    "                target += self.gamma * torch.max(next_q_values[i])\n",
    "            q_values_target[i][actions[i]] = target\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, q_values_target)\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay           \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.remember(state, action, reward, next_state, done)\n",
    "        self.replay(32)\n",
    "        \n",
    "    def train(self, env, episodes, batch_size):\n",
    "        for e in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                self.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "            print(f'Episode {e + 1}/{episodes}')\n",
    "            self.save('model.h5')\n",
    "        env.close()\n",
    "        \n",
    "    def test(self, env):\n",
    "        self.load('model.h5')\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "        env.close()\n",
    "        \n",
    "    def play(self, env):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "        env.close()\n",
    "        \n",
    "    def evaluate(self, env, episodes):\n",
    "        self.load('model.h5')\n",
    "        rewards = []\n",
    "        for e in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            rewards.append(total_reward)\n",
    "        env.close()\n",
    "        return rewards  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
