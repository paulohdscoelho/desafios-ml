{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar um agente de aprendizado que otimize a experiência do usuário neste site, aumentando o número de conversões por meio de recomendações personalizadas, podemos adotar uma abordagem baseada em Aprendizado por Reforço, mais especificamente, Reforço Profundo (Deep Reinforcement Learning) utilizando o Processo de Decisão de Markov (MDP).\n",
    "\n",
    "Irei descrever brevemente como modelar esse problema, considerando suposições e estratégias sobre os dados e comportamento dos usuários, bem como para coleta de dados adicionais.\n",
    "\n",
    "### 1. Definição do Problema\n",
    "\n",
    "O objetivo é criar um agente que aprenda a recomendar produtos ou conteúdos aos usuários, maximizando o número de conversões (transações). As recomendações devem ser personalizadas com base no comportamento de navegação e compras dos usuários no site.\n",
    "\n",
    "### 2. Modelagem do Problema\n",
    "\n",
    "A modelagem do problema envolve a definição de variáveis que podem ser entendidas como estados, ações e funções de recompensa, modelando Cadeias de Markov e aplicando o MDP sobre tais cadeias. Vamos descrever essa abordagem de forma prática e acessível.\n",
    "\n",
    "#### Ambiente\n",
    "\n",
    "O ambiente é o site de e-commerce, onde os estados são definidos pelas interações dos usuários com o site. Cada interação inclui informações como:\n",
    "\n",
    "* Páginas visitadas;\n",
    "* Produtos visualizados;\n",
    "* Tempo gasto em cada página;\n",
    "* Dispositivo e sistema operacional utilizado;\n",
    "* Histórico de transações.\n",
    "\n",
    "#### Estados\n",
    "\n",
    "Os estados podem ser representados por um vetor de características que descrevem o comportamento atual do usuário. Por exemplo:\n",
    "\n",
    "* Produtos recentemente visualizados;\n",
    "* Categorias de interesse;\n",
    "* Dispositivo utilizado (iOS, Android, Desktop);\n",
    "* Tempo de permanência no site;\n",
    "* Navegações anteriores (páginas visitadas, produtos adicionados ao carrinho).\n",
    "\n",
    "#### Ações\n",
    "\n",
    "As ações são as recomendações feitas pelo agente. Isso pode incluir:\n",
    "\n",
    "* Recomendação de produtos específicos;\n",
    "* Ofertas personalizadas;\n",
    "* Sugestão de conteúdo relevante (artigos, reviews);\n",
    "* Recompensas.\n",
    "\n",
    "### 3. Função de Recompensa\n",
    "\n",
    "A função de recompensa deve incentivar ações que levam a uma conversão. Por exemplo:\n",
    "\n",
    "* +1 para cada conversão (transação completada);\n",
    "* -0.1 para cada recomendação que não resulta em clique;\n",
    "* +0.5 para ações que aumentam o tempo de permanência no site.\n",
    "\n",
    "### 4. Suposições\n",
    "\n",
    "*Dados Históricos:* Supomos que temos acesso a um histórico detalhado das interações dos usuários com o site.\n",
    "*Dados em Tempo Real:* É necessário coletar dados em tempo real para ajustar as recomendações dinamicamente.\n",
    "*Capacidade de Personalização:* O site tem a capacidade de personalizar a interface e as recomendações com base nas ações do agente.\n",
    "\n",
    "### 5. Coleta de Dados Adicionais\n",
    "\n",
    "Para melhorar as recomendações, poderíamos coletar dados adicionais como:\n",
    "\n",
    "* Feedback explícito dos usuários sobre recomendações (curtidas, descurtidas);\n",
    "* Dados de contexto (localização geográfica, hora do dia);\n",
    "* Comportamento em redes sociais (se o usuário se conecta via redes sociais);\n",
    "* Informações como o score de crédito e outros dados relevantes obtidos na API do Sistema de Proteção ao Créito (SPC).\n",
    "\n",
    "### 6. Implementação da Função de Recompensa\n",
    "A função de recompensa pode ser implementada como:\n",
    "\n",
    "```python\n",
    "def calcular_recompensa(transacao_completa, recomendacao_clicada, tempo_permanencia):\n",
    "    recompensa = 0\n",
    "    if transacao_completa:\n",
    "        recompensa += 1\n",
    "    if recomendacao_clicada:\n",
    "        recompensa += 0.5\n",
    "    else:\n",
    "        recompensa -= 0.1\n",
    "    if tempo_permanencia > tempo_limiar:\n",
    "        recompensa += 0.5\n",
    "    return recompensa\n",
    "\n",
    "```\n",
    "\n",
    "#### 7. Estrutura do Agente de Aprendizado\n",
    "\n",
    "Para treinar o agente, podemos usar uma Rede Neural Profunda que aprende a mapear estados para ações maximizando as recompensas esperadas. Algoritmos de aprendizado por reforço como Q-Learning, Deep Q-Networks (DQN), Policy Gradient, entre outros, podem ser adaptados para o nosso problema.\n",
    "\n",
    "### 8. Avaliação e Otimização\n",
    "\n",
    "Após treinar o agente, é necessário avaliar seu desempenho em um ambiente simulado ou em produção. Podemos usar métricas como Taxa de Conversão, Tempo Médio de Permanência e Taxa de Cliques. O agente deve ser continuamente otimizado com base no feedback dos usuários, nos dados coletados e nas métricas de desempenho.\n",
    "\n",
    "A implementação do agente em um ambiente real pode ser feita através de contâiners Docker, APIs RESTful e integração com o sistema de recomendação do site. É importante notar que, em um ambiente real, campanhas como Black Friday, Dia das Mães e outras podem influenciar no comportamento dos usuários e, portanto, o agente deve ser capaz de se adaptar a essas mudanças.\n",
    "\n",
    "### 9. Algoritmo de Treinamento\n",
    "\n",
    "1. **Inicialização:** Inicialize a rede neural com pesos aleatórios.\n",
    "2. **Exploração vs Exploração:** Utilize uma política ε-greedy para balancear entre exploração (experimentar novas ações) e exploração (usar ações conhecidas).\n",
    "3. **Atualização da Q-Function:** A cada interação, atualize a Q-Function usando o algoritmo Q-Learning:\n",
    "\n",
    "```python\n",
    "    Q(s, a) = Q(s, a) + α [r + γ max_a Q(s, a) - Q(s, a)]\n",
    "```\n",
    "4. **Replay Memory:** Utilize uma memória de replay para armazenar interações e amostrar mini-batches para treinamento.\n",
    "\n",
    "### 10. Conclusão\n",
    "\n",
    "Implementar um agente de aprendizado por reforço para recomendar produtos em um site de e-commerce pode aumentar significativamente o número de conversões. Ao utilizar uma combinação de dados históricos, dados em tempo real e um modelo de aprendizado profundo, é possível personalizar a experiência do usuário de forma eficaz, adaptando-se continuamente ao comportamento dos clientes e otimizando as recomendações para maximizar as vendas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de Implementação\n",
    "\n",
    "Aqui irei apresentar um esboço de como seria a modelagem de um agente e estrutura de aprendizado por reforço para recomendação de produtos a partir dos dados obtidos da tabela BigQuery fornecida.\n",
    "\n",
    "Por uma limitação de infraestrutura, não será possível realizar o treinamento do agente, mas irei descrever o processo de coleta e preparação dos dados, bem como a estrutura do agente e a função de recompensa, indicando como seria a implementação prática.\n",
    "\n",
    "### 1. Coletar e preparar os dados\n",
    "\n",
    "Primeiro, vamos obter os dados da tabela do BigQuery e prepará-los para serem utilizados no ambiente de aprendizado por reforço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq as gbq\n",
    "import pandas as pd\n",
    "\n",
    "dataset_store = \"bigquery-public-data.google_analytics_sample.ga_sessions_*\"\n",
    "PROJECT_ID = \"testebrius\"\n",
    "REGION = \"US\"\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "    geoNetwork.country AS country,\n",
    "    geoNetwork.city AS city,\n",
    "    device.deviceCategory AS device_category,  \n",
    "    trafficSource.source AS traffic_source,\n",
    "    totals.transactionRevenue AS revenue,\n",
    "    totals.pageviews AS pageviews,\n",
    "    totals.timeOnSite AS time_on_site,\n",
    "    totals.transactions AS transactions,\n",
    "    totals.timeOnScreen AS time_on_screen    \n",
    "FROM {dataset_store}\n",
    "\"\"\"\n",
    "\n",
    "env_df = gbq.read_gbq(sql, project_id=PROJECT_ID)\n",
    "\n",
    "# faz one hot encoding das variáveis categóricas\n",
    "env_df = pd.get_dummies(\n",
    "    env_df, columns=[\"country\", \"city\", \"device_category\", \"traffic_source\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modelagem do Ambiente\n",
    "\n",
    "Iremos modelar o ambiente utilizando a biblioteca `gymnasium` do Python, que permite criar ambientes de aprendizado por reforço.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class EcommerceEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(EcommerceEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.action_space = spaces.Discrete(data.shape[1] - 1)  # Ações baseadas no número de características\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(data.shape[1] - 1,), dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.data.iloc[self.current_step, 1:].values\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.current_step = 0\n",
    "            \n",
    "        reward = self.data.iloc[self.current_step]['revenue']\n",
    "        done = self.current_step == len(self.data) - 1\n",
    "        obs = self.data.iloc[self.current_step, 1:].values\n",
    "        \n",
    "        return obs, reward, done, {}\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "        \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "#instanciar o ambiente usando o DataFrame do BigQuery\n",
    "env = EcommerceEnv(env_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementação do Agente DQN\n",
    "\n",
    "Irei implementar um agente DQN utilizando o framework PyTorch, devido à sua flexibilidade e eficiência para treinar redes neurais profundas e à minha própria experiência com a biblioteca.\n",
    "\n",
    "Além disso, o uso de Pytorch tem a vantagem de ser facilmente integrável com o framework Pytorch Lightning, que facilita o treinamento de modelos complexos e a implementação de pipelines de treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def train_dqn(env, num_episodes=1000, batch_size=64, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    policy_net = DQN(input_dim, output_dim)\n",
    "    target_net = DQN(input_dim, output_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    memory = deque(maxlen=10000)\n",
    "    \n",
    "    epsilon = epsilon_start\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for t in range(200): \n",
    "            if random.random() < epsilon:\n",
    "                action = random.randrange(output_dim)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action = policy_net(torch.FloatTensor(state)).argmax().item()\n",
    "                    \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            if len(memory) > batch_size:\n",
    "                batch = random.sample(memory, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                states = torch.FloatTensor(states)\n",
    "                actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                next_states = torch.FloatTensor(next_states)\n",
    "                dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "                \n",
    "                q_values = policy_net(states).gather(1, actions)\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "                target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "                \n",
    "                loss = criterion(q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    return policy_net\n",
    "\n",
    "# Treinar o DQN\n",
    "trained_policy = train_dqn(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Disponibilizar o Agente em Produção\n",
    "\n",
    "Para colocar o agente treinado em produção em um contêiner Docker, com uma API RESTful que permita a interação com o modelo é necessário seguir os seguintes passos:\n",
    "\n",
    " **1. Salvar o Modelo Treinado:** Primeiro, precisamos salvar o modelo treinado em um arquivo;\n",
    "\n",
    " **2. Criar uma API para Interagir com o Modelo:** Usar uma biblioteca como Flask para criar uma API que permita interagir com o modelo;\n",
    " \n",
    " **3.Criar um Dockerfile:** Configurar um Dockerfile que instala todas as dependências necessárias e configura o ambiente;\n",
    " \n",
    " **4. Construir e Executar o Contêiner Docker:** Construir a imagem Docker e executar o contêiner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# salvar o modelo treinado\n",
    "torch.save(trained_policy.state_dict(), 'dqn_model.pth')\n",
    "\n",
    "input_dim, output_dim = env.observation_space.shape[0], env.action_space.n\n",
    "\n",
    "# Carregar o modelo treinado\n",
    "model = DQN(input_dim, output_dim)\n",
    "model.load_state_dict(torch.load('dqn_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    state = np.array(data['state'])\n",
    "    with torch.no_grad():\n",
    "        action = model(torch.FloatTensor(state)).argmax().item()\n",
    "    return jsonify({'action': action})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os demais passos para subir o Docker, como criação do Dockerfile, configuração do ambiente, criação da API e execução do contêiner, não serão abordados neste notebook, mas são etapas essenciais para disponibilizar o agente em produção, e são habilidades essenciais para um profissional de Data Science imerso na cultura DevOps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
